# SE world 3

library(mgcv) # Necessary for the prediction stage

load('/scratch/rw307/Reading/Scripts/Final_versions/NorthEast/newupd2_for_NE_IH_scripts_241014.RData') # The station data - Need to make sure I have the noise values stored in this dataframe that will need to be added on to the predictions 

# Load the model that the predictions need to be made from:

load('/scratch/rw307/Reading/Scripts/NorthEast/Cluster_scripts/Model_outputs/Nolakes/modelnecoast5gf3other.RData')

# Don't need the 20s or the as:

Predictset=newupd2 # I'll have to make sure that this is updated to be the correct name from the loaded file
rm(newupd2) # Remove the dataframe newup as I don't want two massive datasets floating around

Predictset$Station=sort(rep(1:207,30680))

Predictset$noise=Predictset$pred5other-Predictset$lsmodel40b
# pred5th1 = 60 - raw preds
# pred5th1gam = unsmoothed noise - raw preds (i.e. this is the unsmoothed noise that will be added on because the mean has been subtracted)
# lsmodel...= 60 - (smoothed noise + (60 - pred5th1)) = 60 - (smoothed noise + 60 - (60 - raw preds)) = 60 - (smoothed noise + raw preds) = 60 - raw preds - smoothed noise = pred5th1 - smoothed noise => smoothed noise = pred5th1 - lsmodel... 

# Make an extra row that will be storing the new predictions (with added noise)

Predictset$Inhomogeneous=NA

Predictsetf=Predictset[Predictset$Station_Realisation==6,] # Need to create this dataframe otherwise my new predictions and my old dataframe won't have the same number of rows...

# Want to be able to store the location of the inhomogeneities (and their types) not sure how many there will be at each station => make the storage dataframe much bigger than it should need to be (i.e. Suppose 10 inhomogeneities at ALL stations - so as long as they don't all have more than ten it should be OK)

Inhomogeneity=as.data.frame(matrix(nrow=2070,ncol=5))
names(Inhomogeneity)=c('Station','Location','Type','Method','Size_implemented')

num1=1 # This can be updated as I need to keep a running total of what number inhomogeneity I am on overall so I can add to the appropriate row of the data frame (of course if I want clustered inhomogeneities I'll have to still give that some thought of how to correctly add these in)

for (i in 1:207){

	stat=Predictset[Predictset$Station==i & Predictset$Station_Realisation==6,] # This is where the station will end up

	# Run Poisson process to decide location of inhomogeneities:

	loc=round(14610*cumsum(rexp(15,3))) # I'm multiplying by the number of days an IH can occur on 
	loca=na.omit(stat[loc,14]) # Get just the elements that are less than the length of the time series - NEED TO MAKE SURE
	# YOU'RE LOOKING AT THE TIME COLUMN OTHERWISE NAS IN OTHER COLUMNS WILL CAUSE PROBLEMS! 
	
	loca=loca[loca<=14610] # So that inhomogeneities can't occur in the last two years of the record

	loca=sort(loca,decreasing=TRUE) # This just means that my 'last' IH will get implemented first so that they won't
	# interfere with each other in a way I don't like

	# Use a RNG to decide type of inhomgogeneity:

	# Provisionally if we think abrupt changes are more common than gradual:

	n=length(loca) # The number of inhomogeneities we have
	n1=length(loca) # Just so  I'm not using the same index in 2 loops (even though they should be indexed on the same thing!)

	inhomog=runif(n,0,1) # The random number for each inhomogeneity

        t=runif(n,0,1) # Deciding how the inhomogeneity will be implemented 

	# If the explan var effect is going to be amplified this can be done within the individual bits in the loop...

	if (n==0) { # A quick failsafe (I hope) for if there are no inhomogeneities

		stat = stat

	} else { for (j in 1:n){

				p=loca[j] # So that I have the point at which an inhomogeneity occurs:
	
				if (t[j] <= 0.3){ # Making explanatory variable IH addition more likely than constant offset

					sc=sample(x=c(1.5,1.25,1,.75,.5,.25,-.25,-.5,-.75,-1,-1.25,-1.5),size=1)

					stat$noise[1:p]=stat$noise[1:p]+sc # Shelter change can have a +ve or -ve effect
	
				} else { 

					if (inhomog[j]<=0.5){ # Shelter change - could add another loop to decide size and
					# direction
					
						sc=sample(x=c(.85,.9,.95,1.05,1.1,1.15),size=1)

						stat$uw[1:p]=sc*stat$uw[1:p]
						stat$vw[1:p]=sc*stat$vw[1:p]
						stat$sun[1:p]=sc*stat$sun[1:p] # Della-Marta and Wanner 2006
	
					}

					if (inhomog[j] > 0.5 ){ # Station Relocation 

						sr=stat$Station_Realisation[p]
	
						statn=Predictset[Predictset$Station==i & Predictset$Station_Realisation!=sr,]
		
						ns=stat$noise
	
						stat[1:p,]=statn[1:p,]

						stat$noise=ns # To make sure that the underlying predictions are the only things 
						# that are changing - though I suspect this will result in not enough change and 
						# the amplification/ dampening of explanatory vars will also be necessary

						# Need to have an sc (size of change) to store - can't make it NA

						# Can also choose to amplify some of the changes in explanatory variables:
						sc=0 #sample(x=c(.95,1.05),size=1) 
#						stat$uw[1:p]=sc*stat$uw[1:p]
#						stat$vw[1:p]=sc*stat$vw[1:p]
#						stat$sun[1:p]=sc*stat$sun[1:p]
#						stat$pwc[1:p]=sc*stat$pwc[1:p]

					}

	
				}
	
			Inhomogeneity$Station[(num1)]=stat$Station[1]
			Inhomogeneity$Location[(num1)]=loca[j]
			Inhomogeneity$Type[(num1)]=inhomog[j]
			Inhomogeneity$Method[(num1)]=t[j]
			Inhomogeneity$Size_implemented[(num1)]=sc

			num1=num1+1
		
		}

	}

	statnew=(60-predict.gam(modelnecoast5gf3other,newdata=stat,type="response"))

	Predictsetf[Predictsetf$Station==i,24]=statnew-stat$noise # The 23 here needs to be reset to be whichever column is the
	# column called Inhomogeneous	

}

# This preliminary work is looking promising => SAVE IT:

Inhomogeneity=na.omit(Inhomogeneity) # So I don't have all the extra rows that I initially created to store my values in 

Predictsetf$Difference=Predictsetf$Inhomogeneous-Predictsetf$lsmodel40b

Predictsetf1=Predictsetf # Just in case something goes wrong!

load('/scratch/rw307/Reading/Scripts/NorthEast/neupdatefeb29gam.RData')

TMEANl=neupdatefeb29gam[neupdatefeb29gam$Station!=103,]
TMEANl=TMEANl[TMEANl$Station!=126,] # Getting rid of the two stats I don't have in my predicted smaller world

load('/scratch/rw307/Reading/Scripts/SouthEast/secoastlattime.RData') # So I can select how to make the data missing from these stats
secoastlat=secoastlattime[with(secoastlattime,order(Station)),]

TMEAN=as.data.frame(matrix(nrow=3175380,1))
TMEAN[1:2239640,1]=TMEANl$TMEAN

sstat=sort(sample(1:155,size=61,replace=FALSE))

for (i in 1:61){
	TMEAN[((i+145)*15340+1):((i+146)*15340),1]=secoastlat$TMEAN[secoastlat$Station==sstat[i]]
}

save(sstat,TMEAN,file='Missing_data_in_world3_NE_241014.RData')

Predictsetf1$TMEAN=TMEAN

Predictsetf1[is.na(Predictsetf1$TMEAN),24] <- NA # Need to make sure that Predictsetf actually HAS a TMEAN column for this to work!
Predictsetf1$Difference=Predictsetf1$Inhomogeneous-Predictsetf1$lsmodel40b

save(Inhomogeneity,Predictsetf,Predictsetf1,file="NE_world3_IHs.RData") 

Inhomogeneity1=Inhomogeneity # This way I have a copy of my original dataframe in case anything goes wrong!

# The size of an inhomogeneity over a time-period will be the difference between the inhomogeneous series and the clean series (ls...) (probably averaged) over that time period
# Think about whether people are going to homogenise relative to the last homogeneous sub-period or the period next to where they are - my inhomogeneities are going to stack one on top of each other, so if someone doesn't find one I need to think about what that will look like for them, so I know how to assess it...

Inhomogeneity1$MEAN=NULL
Inhomogeneity1$MEDIAN=NULL

Inhomogeneitysub1=NULL

for (s in 1:207){

	stat=Predictsetf1[Predictsetf1$Station==s,]	
	Inhomogeneitysub=Inhomogeneity1[Inhomogeneity1$Station==s,]
	Inhomogeneitysub=Inhomogeneitysub[with(Inhomogeneitysub,order(Location)),]
	t=nrow(Inhomogeneitysub)

	if (t==0){
	#If this is true (i.e. t=0) I don't want anything to happen - so I'll see if this does what I expect...
	}

	if (t==1){

		Inhomogeneitysub$MEAN[t]=mean(stat$Difference[1:(Inhomogeneitysub$Location[t])],na.rm=TRUE)
		Inhomogeneitysub$MEDIAN[t]=median(stat$Difference[1:(Inhomogeneitysub$Location[t])],na.rm=TRUE)
		Inhomogeneitysub$Length[t]=length(stat$Difference[1:(Inhomogeneitysub$Location[t])])

	} 
	
	if (t>1) {

		for (u in 1:t){

			if (u==1){

				Inhomogeneitysub$MEAN[u]=mean(stat$Difference[1:(Inhomogeneitysub$Location[u])],na.rm=TRUE)
				Inhomogeneitysub$MEDIAN[u]=median(stat$Difference[1:(Inhomogeneitysub$Location[u])],na.rm=TRUE)
				Inhomogeneitysub$Length[u]=length(stat$Difference[1:(Inhomogeneitysub$Location[u])])

			} 
		
			if (u >1){
	
			Inhomogeneitysub$MEAN[u]=mean(stat$Difference[(Inhomogeneitysub$Location[u-1]):(Inhomogeneitysub$Location[u])],na.rm=TRUE)	
			Inhomogeneitysub$MEDIAN[u]=median(stat$Difference[(Inhomogeneitysub$Location[u-1]):(Inhomogeneitysub$Location[u])],na.rm=TRUE)				
			Inhomogeneitysub$Length[u]=length(stat$Difference[(Inhomogeneitysub$Location[u-1]):(Inhomogeneitysub$Location[u])])	
				
			}
		}	
	}

	Inhomogeneitysub1=rbind(Inhomogeneitysub1,Inhomogeneitysub)

}

save(Inhomogeneitysub1,file='Inhomogeneitysub1_NE_world3.RData') 


